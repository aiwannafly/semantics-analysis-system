{
  "Model_Language_Lang": {
    "predicted": {
      "incorrect": {
        "count": 4,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(DeBERTa) Language (англоязычный)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) Language (русского языка)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(kazzand/ru-longformer-tiny-16384) Language (русского языка)"
          },
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(модель) Language (английском языке)"
          }
        ]
      },
      "correct": {
        "count": 5,
        "examples": [
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (зулу)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (английском языке)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (хмонг)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (шотландский гэльский)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (гуарани)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 2,
        "examples": [
          {
            "text": "Есть еще более простой выход из ситуации: если приложение ориентировано на использование только определенных языков – например, русского и английского – устанавливается ясная и строгая директива модели: обрабатывать запросы исключительно на этих языках. Можно внести четкое указание в системный промпт: “Эта модель предназначена для работы только с русским и английским языками. Ты не можешь принимать запросы ни на каких других языка, кроме русского и английского”. Такой подход использует базовый навык моделей следовать предоставленным инструкциям и создает барьер для запросов, выполненных на любых других языках. Эта мера, простая в реализации, может стать первой линией защиты от несанкционированных манипуляций и укрепит безопасность приложений, в которых используются большие языковые модели.",
            "relation": "(большие языковые модели) Language (русского)"
          },
          {
            "text": "Есть еще более простой выход из ситуации: если приложение ориентировано на использование только определенных языков – например, русского и английского – устанавливается ясная и строгая директива модели: обрабатывать запросы исключительно на этих языках. Можно внести четкое указание в системный промпт: “Эта модель предназначена для работы только с русским и английским языками. Ты не можешь принимать запросы ни на каких других языка, кроме русского и английского”. Такой подход использует базовый навык моделей следовать предоставленным инструкциям и создает барьер для запросов, выполненных на любых других языках. Эта мера, простая в реализации, может стать первой линией защиты от несанкционированных манипуляций и укрепит безопасность приложений, в которых используются большие языковые модели.",
            "relation": "(большие языковые модели) Language (английского)"
          }
        ]
      },
      "found": {
        "count": 5,
        "examples": [
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (зулу)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (английском языке)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (хмонг)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (шотландский гэльский)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (гуарани)"
          }
        ]
      }
    }
  },
  "Activity_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Task_isSolvedIn_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Date_isDateOf_Method": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(2021) isDateOf (Rotary Position Embedding)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "InfoResource_Language_Lang": {
    "predicted": {
      "incorrect": {
        "count": 2,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(SOTA) Language (англоязычный)"
          },
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(Википедия) Language (английском языке)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(Википедии) Language (английском языке)"
          }
        ]
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_hasAuthor_Person": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Environment_isUsedIn_Library": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isModificationOf_Model": {
    "predicted": {
      "incorrect": {
        "count": 6,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(LLM) isModificationOf (GPT-NeoX)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(RoFormer) isModificationOf (Mistral)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(RoFormer) isModificationOf (LLM)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(RoFormer) isModificationOf (Llama)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(ruRoPEBert-e5-base-2k) isModificationOf (e5)"
          },
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(RoPEBert) isModificationOf (ai-forever/ruRoberta-large)"
          }
        ]
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(RoPEBert) isModificationOf (ai-forever/ruBert-base)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(RoPEBert) isModificationOf (ai-forever/ruBert-base)"
          }
        ]
      }
    }
  },
  "Organization_isAlternativeNameFor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Application_isUsedForSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Включение дополнительных слов проверки запросов на малоресурсных языках может предотвратить обработку вредоносных инструкций. Подходы вроде применения инструментов типа Lakera Guard могут служить эффективным средством предотвращения выполнения вредоносных запросов. Реализуя механизмы перехвата опасных команд еще до активации модели, можно существенно повысить уровень цифровой безопасности.",
            "relation": "(Lakera Guard) isUsedForSolving (перехвата опасных команд)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Включение дополнительных слов проверки запросов на малоресурсных языках может предотвратить обработку вредоносных инструкций. Подходы вроде применения инструментов типа Lakera Guard могут служить эффективным средством предотвращения выполнения вредоносных запросов. Реализуя механизмы перехвата опасных команд еще до активации модели, можно существенно повысить уровень цифровой безопасности.",
            "relation": "(Lakera Guard) isUsedForSolving (перехвата опасных команд)"
          }
        ]
      }
    }
  },
  "Method_isUsedIn_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isAlternativeNameFor_Model": {
    "predicted": {
      "incorrect": {
        "count": 14,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(LLM) isAlternativeNameFor (модель)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(LLM) isAlternativeNameFor (модель)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5) isAlternativeNameFor (e5-base)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Большие языковые модели) isAlternativeNameFor (нейронные сети)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(LLM) isAlternativeNameFor (нейронные сети)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Большие языковые модели) isAlternativeNameFor (LLM)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Large Language Models) isAlternativeNameFor (нейронные сети)"
          },
          {
            "text": "Про LLM знают почти все, но не очень многим известно, что эти модели могут быть уязвимы, причем данном случае речь идет не о сбоях в работе модели или выдаче неверного ответа, речь о целенаправленных атаках злоумышленников с целью получения конфиденциальной информации, утечкам данных (много и интересно про уязвимости в статье), выдачи вредоносного контента, например, рекомендации посетить нежелательный сайт, а также манипуляции и дезинформации. В LLM могут быть заложены предвзятости, обусловленные предвзятостью в наборах данных для обучения, а это может привести к дискриминационной, ошибочной или предвзятой генерации контента. Большие языковые модели могут стать мишенью для специфических программных атак, нацеленных на эксплуатацию уязвимостей в алгоритмах или инфраструктуре, что может нарушить их функционирование или повлиять на качество генерируемого контента.",
            "relation": "(LLM) isAlternativeNameFor (модель)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(трансформер) isAlternativeNameFor (LM)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(трансформера) isAlternativeNameFor (языковых моделей)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(трансформер) isAlternativeNameFor (языковых моделей)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(трансформера) isAlternativeNameFor (LM)"
          },
          {
            "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
            "relation": "(GPT) isAlternativeNameFor (модель)"
          },
          {
            "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
            "relation": "(GPT) isAlternativeNameFor (языковых моделей)"
          }
        ]
      },
      "correct": {
        "count": 4,
        "examples": [
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isAlternativeNameFor (LLM)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(LLM) isAlternativeNameFor (Large Language Models)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Large Language Models) isAlternativeNameFor (Большие языковые модели)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(языковых моделей) isAlternativeNameFor (LM)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 4,
        "examples": [
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isAlternativeNameFor (LLM)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(LLM) isAlternativeNameFor (Large Language Models)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Large Language Models) isAlternativeNameFor (Большие языковые модели)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(языковых моделей) isAlternativeNameFor (LM)"
          }
        ]
      }
    }
  },
  "Library_isUsedIn_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_hasAuthor_Person": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "За основу бенчмарка качества был выбран encodechka от Дэвида Дале, так как он обладает единым и простым интерфейсом запуска тестов, в нём есть немало современных моделей, а главное, что оценка производится исключительно на русскоязычных датасетах. К тому же он даёт возможность сравнить модели по их способности в семантическое представление предложений, что является важной целью данного проекта.",
            "relation": "(encodechka) hasAuthor (Дэвида Дале)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "За основу бенчмарка качества был выбран encodechka от Дэвида Дале, так как он обладает единым и простым интерфейсом запуска тестов, в нём есть немало современных моделей, а главное, что оценка производится исключительно на русскоязычных датасетах. К тому же он даёт возможность сравнить модели по их способности в семантическое представление предложений, что является важной целью данного проекта.",
            "relation": "(encodechka) hasAuthor (Дэвида Дале)"
          }
        ]
      }
    }
  },
  "Method_solves_Task": {
    "predicted": {
      "incorrect": {
        "count": 3,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(Методы позиционного кодирования) solves (расширения контекста)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) solves (вычисления внимания)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(FlashAttention) solves (вычисления внимания)"
          }
        ]
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) solves (вычисления внимания)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) solves (вычисления внимания)"
          }
        ]
      }
    }
  },
  "Application_isUsedIn_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Date_isDateOf_Model": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(2021) isDateOf (RoFormer)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(2021) isDateOf (RoFormer)"
          }
        ]
      }
    }
  },
  "Science_isAlternativeNameFor_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Metric_isAlternativeNameFor_Metric": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_hasAuthor_Person": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Mistral) hasAuthor (Дэвида Дале)"
          }
        ]
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) hasAuthor (Дэвида Дале)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) hasAuthor (Дэвида Дале)"
          }
        ]
      }
    }
  },
  "InfoResource_isAlternativeNameFor_InfoResource": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Metric_hasValue_Value": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isUsedIn_Application": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isUsedIn_Application": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Object_isAlternativeNameFor_Object": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(«пространство эмбеддингов») isAlternativeNameFor (многомерное облако точек)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(«пространство эмбеддингов») isAlternativeNameFor (многомерное облако точек)"
          }
        ]
      }
    }
  },
  "Application_isAlternativeNameFor_Application": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isUsedIn_Library": {
    "predicted": {
      "incorrect": {
        "count": 4,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(FlashAttention) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Fused C++) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Memory-Efficient Attention) isUsedIn (PyTorch)"
          }
        ]
      },
      "correct": {
        "count": 2,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) isUsedIn (PyTorch)"
          },
          {
            "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
            "relation": "(FlashAttention) isUsedIn (PyTorch)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 2,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) isUsedIn (PyTorch)"
          },
          {
            "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
            "relation": "(FlashAttention) isUsedIn (PyTorch)"
          }
        ]
      }
    }
  },
  "Application_hasAuthor_Person": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Date_isDateOf_Application": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isAlternativeNameFor_Method": {
    "predicted": {
      "incorrect": {
        "count": 3,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(Методы позиционного кодирования) isAlternativeNameFor (методов позиционного кодирования)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isAlternativeNameFor (sdpa)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isAlternativeNameFor (torch.nn.functional.scaled_dot_product_attention)"
          }
        ]
      },
      "correct": {
        "count": 2,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(RoPE) isAlternativeNameFor (Rotary Position Embedding)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) isAlternativeNameFor (sdpa)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 2,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(RoPE) isAlternativeNameFor (Rotary Position Embedding)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) isAlternativeNameFor (sdpa)"
          }
        ]
      }
    }
  },
  "Model_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(text-embedding-ada-002) hasAuthor (OpenAI)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(kazzand/ru-longformer-tiny-16384) hasAuthor (MTS)"
          }
        ]
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(text-embedding-ada-002) hasAuthor (OpenAI)"
          }
        ]
      }
    }
  },
  "Metric_isUsedIn_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Person_isAlternativeNameFor_Person": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Dataset_Language_Lang": {
    "predicted": {
      "incorrect": {
        "count": 2,
        "examples": [
          {
            "text": "Авторы перевели каждую опасную инструкцию из набора данных AdvBench Harmful Behaviors на 12 языков трех разных уровней ресурсов, выбрав языки из различных географических мест и языковых групп, чтобы результаты были универсальными. Непереведенные запросы на английском были добавлены как бейзлайн для сравнения.",
            "relation": "(AdvBench Harmful Behaviors) Language (английский)"
          },
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(enwik8) Language (английском языке)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 2,
        "examples": [
          {
            "text": "Авторы перевели каждую опасную инструкцию из набора данных AdvBench Harmful Behaviors на 12 языков трех разных уровней ресурсов, выбрав языки из различных географических мест и языковых групп, чтобы результаты были универсальными. Непереведенные запросы на английском были добавлены как бейзлайн для сравнения.",
            "relation": "(AdvBench Harmful Behaviors) Language (английском)"
          },
          {
            "text": "Небезопасные запросы из набора данных AdvBench были классифицированы по 16 темам, и был проанализирован успех атак в зависимости от тематики и уровня ресурсов языков. С переводом на малоресурсные языки, обход средств безопасности оказался более успешным по всем темам, за исключением материалов, связанных с сексуальным насилием над детьми, где атаки на мало- и среднересурсных языках показали одинаковый успех из-за успешного обхода на тайском языке. Три темы с самым высоким процентом успешных атак через перевод на малоресурсные языки были: терроризм, финансовая манипуляция и дезинформация. ",
            "relation": "(AdvBench) Language (тайском языке)"
          }
        ]
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Application_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Task_isAlternativeNameFor_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isTrainedOn_Dataset": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(модель) isTrainedOn (enwik8)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isAppliedTo_Object": {
    "predicted": {
      "incorrect": {
        "count": 3,
        "examples": [
          {
            "text": "Обфускация - это процесс изменения промпта (входных данных) таким образом, чтобы он выглядел непонятным или запутанным, но при этом сохранял свою семантику (смысл). Обфускация может быть использована для защиты информации или для создания препятствий для анализа и понимания промпта.",
            "relation": "(Обфускация) isAppliedTo (промпт)"
          },
          {
            "text": "Один из самых главных вопросов, которые мы себе задали в процессе исследования — а какая вообще форма у этих облаков точек? Визуализировать их сложно — пространство эмбеддингов очень многомерно, — а методы снижения размерности тут не сильно помогают. Поэтому мы решили использовать анизотропию в качестве нашего «микроскопа». Анизотропия — это мера, показывающая насколько облако точек вытянуто, насколько оно неоднородно. Чем выше это значение, тем сильнее вытягивается пространство.",
            "relation": "(анизотропия) isAppliedTo (эмбеддинг)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессия) isAppliedTo (эмбеддинг)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 4,
        "examples": [
          {
            "text": "Обфускация - это процесс изменения промпта (входных данных) таким образом, чтобы он выглядел непонятным или запутанным, но при этом сохранял свою семантику (смысл). Обфускация может быть использована для защиты информации или для создания препятствий для анализа и понимания промпта.",
            "relation": "(Обфускация) isAppliedTo (промпта)"
          },
          {
            "text": "Один из самых главных вопросов, которые мы себе задали в процессе исследования — а какая вообще форма у этих облаков точек? Визуализировать их сложно — пространство эмбеддингов очень многомерно, — а методы снижения размерности тут не сильно помогают. Поэтому мы решили использовать анизотропию в качестве нашего «микроскопа». Анизотропия — это мера, показывающая насколько облако точек вытянуто, насколько оно неоднородно. Чем выше это значение, тем сильнее вытягивается пространство.",
            "relation": "(методы снижения размерности) isAppliedTo (эмбеддингов)"
          },
          {
            "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
            "relation": "(методы снижения размерности) isAppliedTo (эмбеддингов)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессии) isAppliedTo (эмбеддинги)"
          }
        ]
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Application_isAppliedTo_Object": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_isAlternativeNameFor_Library": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Dataset_isAlternativeNameFor_Dataset": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isUsedForSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 3,
        "examples": [
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5) isUsedForSolving (STS)"
          },
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(RoPEBert) isUsedForSolving (NER)"
          },
          {
            "text": "Откуда берётся эта неоднородность пространства репрезентаций в декодерах, мы пока не знаем, но предполагаем, что это связано с процессом их обучения, задачей предсказания следующего токена и треугольной маской внимания. Это одна из исследовательских задач, которая сейчас стоит перед нами.",
            "relation": "(декодер) isUsedForSolving (предсказания следующего токена)"
          }
        ]
      },
      "correct": {
        "count": 5,
        "examples": [
          {
            "text": "Использовать модель, выбирая соответствующий способ вычисления внимания можно следующим образом:",
            "relation": "(модель) isUsedForSolving (вычисления внимания)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5-base) isUsedForSolving (STS)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (написание кода)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (создание контента)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (перевод текстов)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 2,
        "examples": [
          {
            "text": "Про LLM знают почти все, но не очень многим известно, что эти модели могут быть уязвимы, причем данном случае речь идет не о сбоях в работе модели или выдаче неверного ответа, речь о целенаправленных атаках злоумышленников с целью получения конфиденциальной информации, утечкам данных (много и интересно про уязвимости в статье), выдачи вредоносного контента, например, рекомендации посетить нежелательный сайт, а также манипуляции и дезинформации. В LLM могут быть заложены предвзятости, обусловленные предвзятостью в наборах данных для обучения, а это может привести к дискриминационной, ошибочной или предвзятой генерации контента. Большие языковые модели могут стать мишенью для специфических программных атак, нацеленных на эксплуатацию уязвимостей в алгоритмах или инфраструктуре, что может нарушить их функционирование или повлиять на качество генерируемого контента.",
            "relation": "(LLM) isUsedForSolving (генерации контента)"
          },
          {
            "text": "Откуда берётся эта неоднородность пространства репрезентаций в декодерах, мы пока не знаем, но предполагаем, что это связано с процессом их обучения, задачей предсказания следующего токена и треугольной маской внимания. Это одна из исследовательских задач, которая сейчас стоит перед нами.",
            "relation": "(декодерах) isUsedForSolving (предсказания следующего токена)"
          }
        ]
      },
      "found": {
        "count": 5,
        "examples": [
          {
            "text": "Использовать модель, выбирая соответствующий способ вычисления внимания можно следующим образом:",
            "relation": "(модель) isUsedForSolving (вычисления внимания)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5-base) isUsedForSolving (STS)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (написание кода)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (создание контента)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (перевод текстов)"
          }
        ]
      }
    }
  },
  "Environment_isAlternativeNameFor_Environment": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Metric_isAppliedTo_Method": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isUsedForTraining_Model": {
    "predicted": {
      "incorrect": {
        "count": 2,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(Методы позиционного кодирования) isUsedForTraining (LLM)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессия) isUsedForTraining (языковых моделей)"
          }
        ]
      },
      "correct": {
        "count": 2,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(disentangled attention) isUsedForTraining (DeBERTa)"
          },
          {
            "text": "Ниже приведена таблица с различными оценками модели ruRoPEBert-e5-base-2k, которая была дополнительно дообучена на 512-2048 токенах, на разных длинах контекста с Dynamic-NTK скейлингом и без него.",
            "relation": "(Dynamic-NTK) isUsedForTraining (ruRoPEBert-e5-base-2k)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 4,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) isUsedForTraining (Mistral)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) isUsedForTraining (GPT-NeoX)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) isUsedForTraining (Llama)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) isUsedForTraining (RoFormer)"
          }
        ]
      },
      "found": {
        "count": 2,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(disentangled attention) isUsedForTraining (DeBERTa)"
          },
          {
            "text": "Ниже приведена таблица с различными оценками модели ruRoPEBert-e5-base-2k, которая была дополнительно дообучена на 512-2048 токенах, на разных длинах контекста с Dynamic-NTK скейлингом и без него.",
            "relation": "(Dynamic-NTK) isUsedForTraining (ruRoPEBert-e5-base-2k)"
          }
        ]
      }
    }
  },
  "Library_isAppliedTo_Object": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_isUsedForSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 3,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(FlashAttention) isUsedForSolving (вычисления внимания)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(xformers) isUsedForSolving (вычисления внимания)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(PyTorch) isUsedForSolving (вычисления внимания)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Object_isUsedInSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Activity_isAlternativeNameFor_Activity": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isIncludedIn_Library": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isPartOf_Method": {
    "predicted": {
      "incorrect": {
        "count": 5,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isPartOf (Memory-Efficient Attention)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isPartOf (Fused C++)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isPartOf (FlashAttention)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Memory-Efficient Attention) isPartOf (Fused C++)"
          },
          {
            "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
            "relation": "(нативного Flash) isPartOf (FlashAttention)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_uses_Model": {
    "predicted": {
      "incorrect": {
        "count": 5,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (RoFormer)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (Llama)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (Mistral)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (GPT-NeoX)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (LLM)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Metric_isUsedFor_Model": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Также мы обнаружили, что незадолго до взрывов лосса во время обучения (больная тема всех, кто учит LLM) внутренняя размерность сильно подрастает. Возможно, у нас получится предсказывать взрывы лосса и не тратить вычислительные ресурсы впустую, или вообще победить эти нестабильности, поняв их природу.",
            "relation": "(внутренняя размерность) isUsedFor (LLM)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Также мы обнаружили, что незадолго до взрывов лосса во время обучения (больная тема всех, кто учит LLM) внутренняя размерность сильно подрастает. Возможно, у нас получится предсказывать взрывы лосса и не тратить вычислительные ресурсы впустую, или вообще победить эти нестабильности, поняв их природу.",
            "relation": "(внутренняя размерность) isUsedFor (LLM)"
          }
        ]
      }
    }
  },
  "Object_isPartOf_Object": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(«пространство эмбеддингов») isPartOf (токен)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Dataset_isTrainedForSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isExampleOf_Model": {
    "predicted": {
      "incorrect": {
        "count": 6,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(LLM) isExampleOf (Llama)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) isExampleOf (GPT-NeoX)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) isExampleOf (Llama)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(ruRoPEBert-e5-base-2k) isExampleOf (encodechka)"
          },
          {
            "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
            "relation": "(GPT) isExampleOf (Llama)"
          },
          {
            "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
            "relation": "(Mistral) isExampleOf (GPT)"
          }
        ]
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Mistral) isExampleOf (LLM)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 6,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(DeBERTa) isExampleOf (энкодерах)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(GPT-NeoX) isExampleOf (LLM)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Llama) isExampleOf (LLM)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Claude 3 Opus) isExampleOf (нейронные сети)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(GPT4) isExampleOf (нейронные сети)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Mistral 7B OpenChat) isExampleOf (нейронные сети)"
          }
        ]
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Mistral) isExampleOf (LLM)"
          }
        ]
      }
    }
  }
}