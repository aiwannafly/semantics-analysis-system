{
  "Application_isAppliedTo_Object": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Metric_isUsedIn_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Object_isAlternativeNameFor_Object": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(«пространство эмбеддингов») isAlternativeNameFor (многомерное облако точек)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(«пространство эмбеддингов») isAlternativeNameFor (многомерное облако точек)"
          }
        ]
      }
    }
  },
  "Model_isModificationOf_Model": {
    "predicted": {
      "incorrect": {
        "count": 2,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(RoFormer) isModificationOf (LLM)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(ruRoPEBert-e5-base-2k) isModificationOf (e5)"
          }
        ]
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(RoPEBert) isModificationOf (ai-forever/ruBert-base)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(RoPEBert) isModificationOf (ai-forever/ruBert-base)"
          }
        ]
      }
    }
  },
  "Dataset_isAlternativeNameFor_Dataset": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Metric_hasValue_Value": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isUsedIn_Application": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Object_isUsedInSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Application_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Environment_isAlternativeNameFor_Environment": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Date_isDateOf_Application": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_Language_Lang": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(модели) Language (английском языке)"
          }
        ]
      },
      "correct": {
        "count": 8,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(DeBERTa) Language (англоязычных)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) Language (русского языка)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(kazzand/ru-longformer-tiny-16384) Language (русского языка)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (зулу)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (гуарани)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (хмонг)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (шотландский гэльский)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (английском языке)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 8,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(DeBERTa) Language (англоязычных)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) Language (русского языка)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(kazzand/ru-longformer-tiny-16384) Language (русского языка)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (зулу)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (гуарани)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (хмонг)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (шотландский гэльский)"
          },
          {
            "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
            "relation": "(GPT-4) Language (английском языке)"
          }
        ]
      }
    }
  },
  "InfoResource_isAlternativeNameFor_InfoResource": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(text-embedding-ada-002) hasAuthor (OpenAI)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(kazzand/ru-longformer-tiny-16384) hasAuthor (MTS)"
          }
        ]
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(text-embedding-ada-002) hasAuthor (OpenAI)"
          }
        ]
      }
    }
  },
  "Metric_isAppliedTo_Method": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_solves_Task": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(Методы позиционного кодирования) solves (расширения контекста)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Organization_isAlternativeNameFor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isUsedIn_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Metric_isUsedFor_Model": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Также мы обнаружили, что незадолго до взрывов лосса во время обучения (больная тема всех, кто учит LLM) внутренняя размерность сильно подрастает. Возможно, у нас получится предсказывать взрывы лосса и не тратить вычислительные ресурсы впустую, или вообще победить эти нестабильности, поняв их природу.",
            "relation": "(внутренняя размерность) isUsedFor (LLM)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Также мы обнаружили, что незадолго до взрывов лосса во время обучения (больная тема всех, кто учит LLM) внутренняя размерность сильно подрастает. Возможно, у нас получится предсказывать взрывы лосса и не тратить вычислительные ресурсы впустую, или вообще победить эти нестабильности, поняв их природу.",
            "relation": "(внутренняя размерность) isUsedFor (LLM)"
          }
        ]
      }
    }
  },
  "Model_isAlternativeNameFor_Model": {
    "predicted": {
      "incorrect": {
        "count": 13,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(LLM) isAlternativeNameFor (моделей)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(LLM) isAlternativeNameFor (модели)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5) isAlternativeNameFor (e5-base)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(LLM) isAlternativeNameFor (нейронные сети)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Large Language Models) isAlternativeNameFor (нейронные сети)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Большие языковые модели) isAlternativeNameFor (нейронные сети)"
          },
          {
            "text": "Про LLM знают почти все, но не очень многим известно, что эти модели могут быть уязвимы, причем данном случае речь идет не о сбоях в работе модели или выдаче неверного ответа, речь о целенаправленных атаках злоумышленников с целью получения конфиденциальной информации, утечкам данных (много и интересно про уязвимости в статье), выдачи вредоносного контента, например, рекомендации посетить нежелательный сайт, а также манипуляции и дезинформации. В LLM могут быть заложены предвзятости, обусловленные предвзятостью в наборах данных для обучения, а это может привести к дискриминационной, ошибочной или предвзятой генерации контента. Большие языковые модели могут стать мишенью для специфических программных атак, нацеленных на эксплуатацию уязвимостей в алгоритмах или инфраструктуре, что может нарушить их функционирование или повлиять на качество генерируемого контента.",
            "relation": "(LLM) isAlternativeNameFor (модели)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(трансформер) isAlternativeNameFor (LM)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(трансформеров) isAlternativeNameFor (языковых моделей)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(трансформеров) isAlternativeNameFor (LM)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(трансформер) isAlternativeNameFor (языковых моделей)"
          },
          {
            "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
            "relation": "(GPT) isAlternativeNameFor (языковых моделей)"
          },
          {
            "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
            "relation": "(GPT) isAlternativeNameFor (модели)"
          }
        ]
      },
      "correct": {
        "count": 5,
        "examples": [
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isAlternativeNameFor (LLM)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(LLM) isAlternativeNameFor (Большие языковые модели)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Large Language Models) isAlternativeNameFor (Большие языковые модели)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(LLM) isAlternativeNameFor (Large Language Models)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(языковых моделей) isAlternativeNameFor (LM)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 5,
        "examples": [
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isAlternativeNameFor (LLM)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(LLM) isAlternativeNameFor (Большие языковые модели)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Large Language Models) isAlternativeNameFor (Большие языковые модели)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(LLM) isAlternativeNameFor (Large Language Models)"
          },
          {
            "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
            "relation": "(языковых моделей) isAlternativeNameFor (LM)"
          }
        ]
      }
    }
  },
  "Metric_isAlternativeNameFor_Metric": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isExampleOf_Model": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Mistral) isExampleOf (LLM)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 6,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(DeBERTa) isExampleOf (энкодерах)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Llama) isExampleOf (LLM)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(GPT-NeoX) isExampleOf (LLM)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Mistral 7B OpenChat) isExampleOf (нейронные сети)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(GPT4) isExampleOf (нейронные сети)"
          },
          {
            "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
            "relation": "(Claude 3 Opus) isExampleOf (нейронные сети)"
          }
        ]
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Mistral) isExampleOf (LLM)"
          }
        ]
      }
    }
  },
  "Activity_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isUsedIn_Application": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_hasAuthor_Person": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "За основу бенчмарка качества был выбран encodechka от Дэвида Дале, так как он обладает единым и простым интерфейсом запуска тестов, в нём есть немало современных моделей, а главное, что оценка производится исключительно на русскоязычных датасетах. К тому же он даёт возможность сравнить модели по их способности в семантическое представление предложений, что является важной целью данного проекта.",
            "relation": "(encodechka) hasAuthor (Дэвида Дале)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "За основу бенчмарка качества был выбран encodechka от Дэвида Дале, так как он обладает единым и простым интерфейсом запуска тестов, в нём есть немало современных моделей, а главное, что оценка производится исключительно на русскоязычных датасетах. К тому же он даёт возможность сравнить модели по их способности в семантическое представление предложений, что является важной целью данного проекта.",
            "relation": "(encodechka) hasAuthor (Дэвида Дале)"
          }
        ]
      }
    }
  },
  "Task_isSolvedIn_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_uses_Model": {
    "predicted": {
      "incorrect": {
        "count": 5,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (Llama)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (GPT-NeoX)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (RoFormer)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (Mistral)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) uses (LLM)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Science_isAlternativeNameFor_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Application_isUsedForSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Включение дополнительных слов проверки запросов на малоресурсных языках может предотвратить обработку вредоносных инструкций. Подходы вроде применения инструментов типа Lakera Guard могут служить эффективным средством предотвращения выполнения вредоносных запросов. Реализуя механизмы перехвата опасных команд еще до активации модели, можно существенно повысить уровень цифровой безопасности.",
            "relation": "(Lakera Guard) isUsedForSolving (перехвата опасных команд)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Включение дополнительных слов проверки запросов на малоресурсных языках может предотвратить обработку вредоносных инструкций. Подходы вроде применения инструментов типа Lakera Guard могут служить эффективным средством предотвращения выполнения вредоносных запросов. Реализуя механизмы перехвата опасных команд еще до активации модели, можно существенно повысить уровень цифровой безопасности.",
            "relation": "(Lakera Guard) isUsedForSolving (перехвата опасных команд)"
          }
        ]
      }
    }
  },
  "Application_isUsedIn_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Date_isDateOf_Method": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(2021) isDateOf (Rotary Position Embedding)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isIncludedIn_Library": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isAppliedTo_Object": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Один из самых главных вопросов, которые мы себе задали в процессе исследования — а какая вообще форма у этих облаков точек? Визуализировать их сложно — пространство эмбеддингов очень многомерно, — а методы снижения размерности тут не сильно помогают. Поэтому мы решили использовать анизотропию в качестве нашего «микроскопа». Анизотропия — это мера, показывающая насколько облако точек вытянуто, насколько оно неоднородно. Чем выше это значение, тем сильнее вытягивается пространство.",
            "relation": "(анизотропию) isAppliedTo (эмбеддингов)"
          }
        ]
      },
      "correct": {
        "count": 2,
        "examples": [
          {
            "text": "Обфускация - это процесс изменения промпта (входных данных) таким образом, чтобы он выглядел непонятным или запутанным, но при этом сохранял свою семантику (смысл). Обфускация может быть использована для защиты информации или для создания препятствий для анализа и понимания промпта.",
            "relation": "(Обфускация) isAppliedTo (промпта)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессии) isAppliedTo (эмбеддинги)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 2,
        "examples": [
          {
            "text": "Один из самых главных вопросов, которые мы себе задали в процессе исследования — а какая вообще форма у этих облаков точек? Визуализировать их сложно — пространство эмбеддингов очень многомерно, — а методы снижения размерности тут не сильно помогают. Поэтому мы решили использовать анизотропию в качестве нашего «микроскопа». Анизотропия — это мера, показывающая насколько облако точек вытянуто, насколько оно неоднородно. Чем выше это значение, тем сильнее вытягивается пространство.",
            "relation": "(методы снижения размерности) isAppliedTo (эмбеддингов)"
          },
          {
            "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
            "relation": "(методы снижения размерности) isAppliedTo (эмбеддингов)"
          }
        ]
      },
      "found": {
        "count": 2,
        "examples": [
          {
            "text": "Обфускация - это процесс изменения промпта (входных данных) таким образом, чтобы он выглядел непонятным или запутанным, но при этом сохранял свою семантику (смысл). Обфускация может быть использована для защиты информации или для создания препятствий для анализа и понимания промпта.",
            "relation": "(Обфускация) isAppliedTo (промпта)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессии) isAppliedTo (эмбеддинги)"
          }
        ]
      }
    }
  },
  "Environment_isUsedIn_Library": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isUsedIn_Library": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 6,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Fused C++) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Memory-Efficient Attention) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(FlashAttention) isUsedIn (PyTorch)"
          },
          {
            "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
            "relation": "(FlashAttention) isUsedIn (PyTorch)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 6,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Fused C++) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Memory-Efficient Attention) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isUsedIn (PyTorch)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(FlashAttention) isUsedIn (PyTorch)"
          },
          {
            "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
            "relation": "(FlashAttention) isUsedIn (PyTorch)"
          }
        ]
      }
    }
  },
  "Method_isAlternativeNameFor_Method": {
    "predicted": {
      "incorrect": {
        "count": 3,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(Методы позиционного кодирования) isAlternativeNameFor (методов позиционного кодирования)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isAlternativeNameFor (sdpa)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isAlternativeNameFor (torch.nn.functional.scaled_dot_product_attention)"
          }
        ]
      },
      "correct": {
        "count": 2,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(RoPE) isAlternativeNameFor (Rotary Position Embedding)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) isAlternativeNameFor (sdpa)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 2,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(RoPE) isAlternativeNameFor (Rotary Position Embedding)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(torch.nn.functional.scaled_dot_product_attention) isAlternativeNameFor (sdpa)"
          }
        ]
      }
    }
  },
  "Application_isAlternativeNameFor_Application": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Activity_isAlternativeNameFor_Activity": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_isUsedIn_Science": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_hasAuthor_Organization": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_isAlternativeNameFor_Library": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_hasAuthor_Person": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Mistral) hasAuthor (Дэвида Дале)"
          }
        ]
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) hasAuthor (Дэвида Дале)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(cointegrated/rubert-tiny2) hasAuthor (Дэвида Дале)"
          }
        ]
      }
    }
  },
  "Dataset_Language_Lang": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 2,
        "examples": [
          {
            "text": "Авторы перевели каждую опасную инструкцию из набора данных AdvBench Harmful Behaviors на 12 языков трех разных уровней ресурсов, выбрав языки из различных географических мест и языковых групп, чтобы результаты были универсальными. Непереведенные запросы на английском были добавлены как бейзлайн для сравнения.",
            "relation": "(AdvBench Harmful Behaviors) Language (английском)"
          },
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(enwik8) Language (английском языке)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 1,
        "examples": [
          {
            "text": "Небезопасные запросы из набора данных AdvBench были классифицированы по 16 темам, и был проанализирован успех атак в зависимости от тематики и уровня ресурсов языков. С переводом на малоресурсные языки, обход средств безопасности оказался более успешным по всем темам, за исключением материалов, связанных с сексуальным насилием над детьми, где атаки на мало- и среднересурсных языках показали одинаковый успех из-за успешного обхода на тайском языке. Три темы с самым высоким процентом успешных атак через перевод на малоресурсные языки были: терроризм, финансовая манипуляция и дезинформация. ",
            "relation": "(AdvBench) Language (тайском языке)"
          }
        ]
      },
      "found": {
        "count": 2,
        "examples": [
          {
            "text": "Авторы перевели каждую опасную инструкцию из набора данных AdvBench Harmful Behaviors на 12 языков трех разных уровней ресурсов, выбрав языки из различных географических мест и языковых групп, чтобы результаты были универсальными. Непереведенные запросы на английском были добавлены как бейзлайн для сравнения.",
            "relation": "(AdvBench Harmful Behaviors) Language (английском)"
          },
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(enwik8) Language (английском языке)"
          }
        ]
      }
    }
  },
  "Library_isUsedForSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isTrainedOn_Dataset": {
    "predicted": {
      "incorrect": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(модели) isTrainedOn (enwik8)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Task_isAlternativeNameFor_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Library_isAppliedTo_Object": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isPartOf_Method": {
    "predicted": {
      "incorrect": {
        "count": 2,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(нативный метод) isPartOf (Fused C++)"
          },
          {
            "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
            "relation": "(нативного Flash) isPartOf (FlashAttention)"
          }
        ]
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 3,
        "examples": [
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Memory-Efficient Attention) isPartOf (нативный метод)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(Fused C++) isPartOf (нативный метод)"
          },
          {
            "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
            "relation": "(FlashAttention) isPartOf (нативный метод)"
          }
        ]
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Method_isUsedForTraining_Model": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 5,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(Методы позиционного кодирования) isUsedForTraining (LLM)"
          },
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(disentangled attention) isUsedForTraining (DeBERTa)"
          },
          {
            "text": "Ниже приведена таблица с различными оценками модели ruRoPEBert-e5-base-2k, которая была дополнительно дообучена на 512-2048 токенах, на разных длинах контекста с Dynamic-NTK скейлингом и без него.",
            "relation": "(Dynamic-NTK) isUsedForTraining (ruRoPEBert-e5-base-2k)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессии) isUsedForTraining (языковых моделей)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессии) isUsedForTraining (трансформеры)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 4,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) isUsedForTraining (Llama)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) isUsedForTraining (GPT-NeoX)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) isUsedForTraining (RoFormer)"
          },
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(Rotary Position Embedding) isUsedForTraining (Mistral)"
          }
        ]
      },
      "found": {
        "count": 5,
        "examples": [
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(disentangled attention) isUsedForTraining (DeBERTa)"
          },
          {
            "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
            "relation": "(Методы позиционного кодирования) isUsedForTraining (LLM)"
          },
          {
            "text": "Ниже приведена таблица с различными оценками модели ruRoPEBert-e5-base-2k, которая была дополнительно дообучена на 512-2048 токенах, на разных длинах контекста с Dynamic-NTK скейлингом и без него.",
            "relation": "(Dynamic-NTK) isUsedForTraining (ruRoPEBert-e5-base-2k)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессии) isUsedForTraining (языковых моделей)"
          },
          {
            "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
            "relation": "(компрессии) isUsedForTraining (трансформеры)"
          }
        ]
      }
    }
  },
  "Object_isPartOf_Object": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Dataset_isTrainedForSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Application_hasAuthor_Person": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "InfoResource_Language_Lang": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(Википедии) Language (английском языке)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
            "relation": "(Википедии) Language (английском языке)"
          }
        ]
      }
    }
  },
  "Method_hasAuthor_Person": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  },
  "Model_isUsedForSolving_Task": {
    "predicted": {
      "incorrect": {
        "count": 2,
        "examples": [
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5) isUsedForSolving (encodechka)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(ruRoPEBert-e5-base-2k) isUsedForSolving (encodechka)"
          }
        ]
      },
      "correct": {
        "count": 8,
        "examples": [
          {
            "text": "Использовать модель, выбирая соответствующий способ вычисления внимания можно следующим образом:",
            "relation": "(модель) isUsedForSolving (вычисления внимания)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5) isUsedForSolving (STS)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5-base) isUsedForSolving (STS)"
          },
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(RoPEBert) isUsedForSolving (NER)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (написание кода)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (перевод текстов)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (создание контента)"
          },
          {
            "text": "Откуда берётся эта неоднородность пространства репрезентаций в декодерах, мы пока не знаем, но предполагаем, что это связано с процессом их обучения, задачей предсказания следующего токена и треугольной маской внимания. Это одна из исследовательских задач, которая сейчас стоит перед нами.",
            "relation": "(декодерах) isUsedForSolving (предсказания следующего токена)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 3,
        "examples": [
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(ai-forever/ruRoberta-large) isUsedForSolving (NER)"
          },
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(ai-forever/ruBert-base) isUsedForSolving (NER)"
          },
          {
            "text": "Про LLM знают почти все, но не очень многим известно, что эти модели могут быть уязвимы, причем данном случае речь идет не о сбоях в работе модели или выдаче неверного ответа, речь о целенаправленных атаках злоумышленников с целью получения конфиденциальной информации, утечкам данных (много и интересно про уязвимости в статье), выдачи вредоносного контента, например, рекомендации посетить нежелательный сайт, а также манипуляции и дезинформации. В LLM могут быть заложены предвзятости, обусловленные предвзятостью в наборах данных для обучения, а это может привести к дискриминационной, ошибочной или предвзятой генерации контента. Большие языковые модели могут стать мишенью для специфических программных атак, нацеленных на эксплуатацию уязвимостей в алгоритмах или инфраструктуре, что может нарушить их функционирование или повлиять на качество генерируемого контента.",
            "relation": "(LLM) isUsedForSolving (генерации контента)"
          }
        ]
      },
      "found": {
        "count": 8,
        "examples": [
          {
            "text": "Использовать модель, выбирая соответствующий способ вычисления внимания можно следующим образом:",
            "relation": "(модель) isUsedForSolving (вычисления внимания)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5-base) isUsedForSolving (STS)"
          },
          {
            "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
            "relation": "(e5) isUsedForSolving (STS)"
          },
          {
            "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
            "relation": "(RoPEBert) isUsedForSolving (NER)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (создание контента)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (перевод текстов)"
          },
          {
            "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
            "relation": "(большие языковые модели) isUsedForSolving (написание кода)"
          },
          {
            "text": "Откуда берётся эта неоднородность пространства репрезентаций в декодерах, мы пока не знаем, но предполагаем, что это связано с процессом их обучения, задачей предсказания следующего токена и треугольной маской внимания. Это одна из исследовательских задач, которая сейчас стоит перед нами.",
            "relation": "(декодерах) isUsedForSolving (предсказания следующего токена)"
          }
        ]
      }
    }
  },
  "Date_isDateOf_Model": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(2021) isDateOf (RoFormer)"
          }
        ]
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 1,
        "examples": [
          {
            "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
            "relation": "(2021) isDateOf (RoFormer)"
          }
        ]
      }
    }
  },
  "Person_isAlternativeNameFor_Person": {
    "predicted": {
      "incorrect": {
        "count": 0,
        "examples": []
      },
      "correct": {
        "count": 0,
        "examples": []
      }
    },
    "expected": {
      "not_found": {
        "count": 0,
        "examples": []
      },
      "found": {
        "count": 0,
        "examples": []
      }
    }
  }
}